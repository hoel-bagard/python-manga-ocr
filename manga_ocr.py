import copy
import logging
from argparse import ArgumentParser
from pathlib import Path

import cv2
import numpy as np
import pytesseract

from src.my_types import OCRData
from src.render_ocr import render_detected_tesseract, render_text
from src.text_bboxes import get_text_bboxes
from src.utils.logger import create_logger
from src.utils.misc import clean_print, show_img


def handle_confidence(ocr_data: OCRData, logger: logging.Logger) -> None:
    """Checks the confidence of the Tesseract output. Exits the program if too low.

    Args:
        ocr_data (OCRData): The data to check.
        logger (Logger): Logger used to print the confidence.
    """
    if len((valid_confs := [conf for conf in ocr_data['conf'] if conf != '-1'])) != 0:
        mean_conf = np.mean(valid_confs)
    else:
        mean_conf = 0
    # If the average confidence is too low, then the result was probably garbage generated by noise.
    if mean_conf < 10:
        logger.info("Did not find any text on the image.")
        exit()
    logger.info(f"Average confidence for the image: {mean_conf:.2f}")


def clean_ocr_results(ocr_data: OCRData) -> OCRData:
    """Remove unwanted characters from the Tesseract output.

    Has the side effect of removing empty lines.

    Args:
        ocr_data: The raw output from the pytesseract.image_to_data function.

    Returns:
        Same type of data, but without the noise.
    """
    clean_ocr_data: OCRData = copy.deepcopy(ocr_data)  # Not necessary but feels safer.
    for i in range(len(ocr_data["text"])-1, -1, -1):
        char = ocr_data["text"][i]
        # Remove empty and special characters  (most likely errors)
        if char == '' or char in "<>{}[];`@#$%^*_=~\\":
            for key in ocr_data.keys():
                clean_ocr_data[key].pop(i)
    return clean_ocr_data


def ocr_char_to_block(ocr_data: OCRData) -> OCRData:
    """Reformats the ocr data to have entries correspond to blocks instead of characters.

    Note: The width and height values can get quite wrong since errors accumulate.

    Args:
        ocr_data: Standard OCR data where most entries correspond to one or two characters.

    Returns:
        Same type of data, but each entry corresponds to a block of characters.
    """
    nb_blocks = max(ocr_data["block_num"])
    # Not sure if there is a better way to instanciate a TypedDict.
    block_ocr_data: OCRData = {
        "level": [4 for _ in range(nb_blocks)],
        "page_num": [1 for _ in range(nb_blocks)],
        "block_num": [i for i in range(1, nb_blocks+1)],
        "line_num": [i for i in range(1, nb_blocks+1)],
        "word_num": [1 for _ in range(nb_blocks)],
        "left": [],
        "top": [],
        "width": [],
        "height": [],
        "conf": [],
        "text": [],
    }
    for i in range(1, nb_blocks+1):
        if i not in ocr_data["block_num"]:
            continue
        block_ocr_data["left"].append(min([ocr_data["left"][j]
                                           for j in range(len(ocr_data["text"])) if ocr_data["block_num"][j] == i]))
        block_ocr_data["top"].append(min([ocr_data["top"][j]
                                          for j in range(len(ocr_data["text"])) if ocr_data["block_num"][j] == i]))
        block_ocr_data["width"].append(max([ocr_data["width"][j]
                                            for j in range(len(ocr_data["text"])) if ocr_data["block_num"][j] == i]))
        block_ocr_data["height"].append(sum([ocr_data["height"][j]
                                             for j in range(len(ocr_data["text"])) if ocr_data["block_num"][j] == i]))
        block_ocr_data["conf"].append(np.mean([float(ocr_data["conf"][j])
                                               for j in range(len(ocr_data["text"])) if ocr_data["block_num"][j] == i]))
        block_ocr_data["text"].append(''.join([ocr_data["text"][j]
                                               for j in range(len(ocr_data["text"])) if ocr_data["block_num"][j] == i]))
    return block_ocr_data


def process_area(img: np.ndarray, logger: logging.Logger, display_images: bool = False):
    # Tesseract seems to (sometimes) work much better on bigger images. I tried changing the DPI, but it didn't help.
    # height, width = img.shape
    # if width < 300:
    #     logger.debug(f"Area had a small width ({width}px), it will be resized.")
    #     img = cv2.resize(img, (2*width, 2*height))

    # Notes on the (py)Tesseract options:
    # For the config --psm, use either 5 or 12:
    #     5  Assume a single uniform block of vertically aligned text.
    #     12 Sparse text with OSD.
    # From my limited testing, 12 is more accurate, but does not separate lines (5 does).
    # For the lang, use either "jpn+jpn_vert" or "jpn_vert".
    # I tried adding more option like --oem 3 -c load_system_dawg=0 load_freq_dawg=0 use_new_state_cost=1
    # but it did not change anything. I might not have done it properly ? (also not sure if the dpi option can be used)
    ocr_data: OCRData = pytesseract.image_to_data(img,
                                                  config="--psm 12",
                                                  lang="jpn_vert",
                                                  output_type=pytesseract.Output.DICT)
    logger.debug(f"Frame processed by Tesseract. Raw output:\n{ocr_data}")

    ocr_data = clean_ocr_results(ocr_data)
    logger.debug(f"Cleaned OCR data:\n{ocr_data}")
    if len(ocr_data["text"]) == 0:  # No text on that tile
        return

    ocr_data = ocr_char_to_block(ocr_data)

    if logger.getEffectiveLevel() == logging.DEBUG and display_images:
        result_img = render_detected_tesseract(np.full(img.shape, 255, dtype=np.uint8), ocr_data, draw_bbox=True)
        show_img(cv2.hconcat([img, result_img]))

    logger.debug("OCR results:\n\t" + "\n\t".join(ocr_data["text"]))
    return ocr_data["text"]


def main():
    parser = ArgumentParser(description="OCR to read manga using Tesseract")
    parser.add_argument("img_path", type=Path, help="Path to the image to process.")
    parser.add_argument("--output_path", "-o", type=Path, default=None, help="Save resulting image there if given.")
    parser.add_argument("--display_images", "-d", action="store_true", help="Displays some debug images.")
    parser.add_argument("--verbose_level", "-v", choices=["debug", "info", "error"], default="info", type=str,
                        help="Logger level.")
    args = parser.parse_args()

    img_path: Path = args.img_path
    output_path: Path = args.output_path
    verbose_level: str = args.verbose_level
    display_images: bool = args.display_images
    logger = create_logger("Manga OCR", verbose_level=verbose_level)

    img = cv2.imread(str(img_path), 0)
    result_img = img.copy()
    if display_images and verbose_level == "debug":
        show_img(img, "Input image")

    logger.info("Looking for potential text areas in the image, this might take a while (~20s).")
    # text_bounding_boxes = get_text_bboxes(img, logger)
    text_bounding_boxes = [(1087, 199, 51, 292), (706, 204, 164, 242), (217, 212, 272, 353), (1397, 526, 87, 154),
                           (338, 726, 95, 129), (934, 744, 69, 143), (201, 940, 45, 145), (539, 1012, 66, 19),
                           (233, 1210, 163, 292), (1492, 1225, 53, 232), (1419, 1235, 42, 163), (1459, 1586, 169, 345),
                           (697, 1625, 135, 231), (1148, 1630, 95, 129), (562, 1738, 31, 48), (188, 1805, 96, 130),
                           (945, 1828, 130, 200), (184, 2068, 107, 194), (1280, 2072, 9, 15), (629, 2082, 210, 220),
                           (409, 2232, 81, 115), (929, 2247, 85, 81), (1465, 2317, 128, 44)]

    logger.info(f"Found {len(text_bounding_boxes)} potential text areas. Now doing OCR on them.")
    padding = 50
    for i, (left, top, width, height) in enumerate(text_bounding_boxes, start=1):
        clean_print(f"Processing area {i}/{len(text_bounding_boxes)}", end="\r")
        logger.debug(f"Processing area with width {width} and height {height}    ({i}/{len(text_bounding_boxes)})")
        # The bouding boxing are pretty tight, add a padding aroung it.
        # Tesseract website: "if you OCR just text area without any border, Tesseract could have problems with it."

        padded_crop = cv2.copyMakeBorder(img[top:top+height, left:left+width],
                                         padding, padding, padding, padding, cv2.BORDER_CONSTANT, value=255)
        # Tried adding erosion (use dilatation function since text is black on white (or do a bitwise not))
        # kernel = np.ones((3, 3), np.uint8)
        # padded_crop = cv2.dilate(padded_crop, kernel, iterations=1)

        detected_text = process_area(padded_crop, logger, display_images)
        if detected_text is not None:
            logger.debug(f"Detected text: {detected_text}")
            # Replace original text by the rendered detected one.
            result_img[top:top+height, left:left+width] = render_text(detected_text, width, height)
    logger.info("Finished processing the image.")

    if display_images:
        show_img(cv2.hconcat([img, result_img]), "Result")
    if output_path is not None:
        logger.info(f"Saved result at {output_path}")
        cv2.imwrite(str(output_path), img)


if __name__ == "__main__":
    main()
